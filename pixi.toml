[project]
name = "nelli-ai-scientist"
version = "0.1.0"
description = "Multi-Agent Framework for Novel Lineage Discovery"
channels = ["conda-forge", "bioconda"]
platforms = ["linux-64", "osx-64", "osx-arm64", "win-64"]

[dependencies]
python = ">=3.9,<3.12"
pip = "*"
nodejs = ">=18"

# Core dependencies
pydantic = ">=2.7.0"
pyyaml = ">=6.0"
redis-py = ">=5.0.0"
python-dotenv = ">=1.0.0"
aiohttp = ">=3.9.0"

# Bioinformatics
biopython = ">=1.83"
numpy = ">=1.26.0"
pandas = ">=2.2.0"

# Development tools
pytest = ">=8.0.0"
pytest-asyncio = ">=0.23.0"
pytest-cov = ">=5.0.0"
black = ">=24.0.0"
mypy = ">=1.10.0"
ruff = ">=0.4.0"

# API frameworks
fastapi = ">=0.111.0"
uvicorn = ">=0.30.0"

# Documentation
mkdocs = ">=1.5.0"
mkdocs-material = ">=9.5.0"

[pypi-dependencies]
openai = ">=1.35.0"
mcp = ">=0.1.0"
# anthropic = ">=0.28.0"  # Optional - only needed for legacy Claude support
asyncio = ">=3.4.3"
fastmcp = ">=1.0"

[tasks]
# Development tasks
test = "pytest tests/ -v --cov=src --cov-report=html"
test-quick = "pytest tests/ -v -x"
format = "black . && ruff check --fix ."
lint = "black --check . && ruff check . && mypy src/ --ignore-missing-imports"
typecheck = "mypy src/ --ignore-missing-imports"

# Documentation
docs = "mkdocs serve"
docs-build = "mkdocs build"

# Agent tasks
agent-test = { cmd = "cd agents/template && pytest tests/ -v", cwd = "." }
agent-run = { cmd = "cd agents/template && python -m src.agent", cwd = "." }
sophisticated-agent = { cmd = "python agents/sophisticated_agent/run_enhanced.py", cwd = "." }
sophisticated-agent-test = { cmd = "cd agents/sophisticated_agent && python test_enhanced_agent.py", cwd = "." }

# MCP tasks
mcp-test = { cmd = "cd mcps/template && pytest tests/ -v", cwd = "." }
mcp-run = { cmd = "cd mcps/template && python -m src.server", cwd = "." }
mcp-client = { cmd = "cd mcps/template && python -m src.client", cwd = "." }

# Integration tasks
integration-test = { cmd = "cd integration && pytest tests/ -v", cwd = "." }
redis-start = "redis-server"

# Setup tasks
setup = "pip install -e ."
clean = "find . -type d -name __pycache__ -exec rm -rf {} + && find . -type f -name '*.pyc' -delete"

# Data download tasks
download-data = """
mkdir -p data && \
cd data && \
echo "ðŸ“¥ Downloading NeLLi hackathon data..." && \
(wget -O nelli_hackathon.tar.gz https://portal.nersc.gov/cfs/nelli/nelli_hackathon.tar.gz || \
 curl -L -o nelli_hackathon.tar.gz https://portal.nersc.gov/cfs/nelli/nelli_hackathon.tar.gz || \
 python -c "
import urllib.request
print('Downloading with Python urllib...')
urllib.request.urlretrieve('https://portal.nersc.gov/cfs/nelli/nelli_hackathon.tar.gz', 'nelli_hackathon.tar.gz')
print('Download completed!')
") && \
echo "ðŸ“¦ Extracting data..." && \
tar -xzf nelli_hackathon.tar.gz && \
echo "ðŸ§¹ Cleaning up..." && \
rm nelli_hackathon.tar.gz && \
echo "âœ… Hackathon data downloaded and extracted to data/ directory" && \
echo "ðŸ“Š Available datasets:" && \
find . -name "*.fa*" -o -name "*.fna" -o -name "*.fastq*" | head -10
"""
download-data-check = """
echo "ðŸ“ Checking for hackathon data..." && \
ls -la data/ 2>/dev/null || echo "âŒ No data directory found. Run 'pixi run download-data' to download hackathon datasets" && \
echo "" && \
echo "ðŸ“Š Sample data files:" && \
find data/ -name "*.fa*" -o -name "*.fna" -o -name "*.fastq*" 2>/dev/null | head -5 || echo "No FASTA files found"
"""

# Context7 MCP server tasks
context7-test = "timeout 5s npx -y @upstash/context7-mcp || echo 'Context7 test completed'"
context7-integration-test = "python test_context7_integration.py"
agent-context7-demo = "timeout 60s python test_agent_context7.py || echo 'Demo completed or timed out'"
agent-context7-verify = "python verify_agent_context7.py"
enhanced-agent-demo = "python enhanced_agent_demo.py"

[feature.dev]
dependencies = { jupyter = "*", ipython = "*", jupyterlab = "*" }

[environments]
default = { solve-group = "default" }
dev = { features = ["dev"], solve-group = "default" }
